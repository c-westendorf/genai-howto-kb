---
title: Position: AI Should Not Be An Imitation Game — Centaur Evaluations
source_id: S0016
type: paper
authors: Andreas Haupt; Erik Brynjolfsson
published: 2025
accessed: 2026-01-17
evidence_grade: B
tags:
  - human+AI
  - evaluation
  - collaboration
url: https://digitaleconomy.stanford.edu/wp-content/uploads/2025/06/CentaurEvaluations.pdf
---

# Why this matters
Your goal is “thought partnership” and “scaling minds.” This paper argues evaluations should measure **human+AI collaboration**, not just model-alone performance.

# Key takeaways (operational)
- Introduces **centaur evaluations**: humans and AI jointly solve tasks.
- Argues centaur evaluations better capture human-centered desiderata like helpfulness and interpretability.
- Provides a structure for centaur benchmarks (human selection, interface, scoring; optionally transcripts).

# How to incorporate
- Use centaur evaluation thinking to design onboarding assessments:
  - grade the *workflow* (prompts, verification, decisions), not only final text.

# Limitations / cautions
- Running centaur evaluations can be costly and hard to replicate; the paper discusses approximations.

# Suggested links
- Pair with S0004 (HELM) for evaluation rigor.
